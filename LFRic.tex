\documentclass[times]{elsarticle}
\bibliographystyle{elsarticle-num}
\usepackage{graphicx,colordvi}
\begin{document}
\begin{frontmatter}

\title{LFRic and PSyclone: Building a Domain Specific Language for Weather and Climate models}

\author[met]{S.~Adams}
\author[hartree]{R.~Ford}
\author[met]{M.~Hambley}
\author[met]{J.M.~Hobson}
\author[met]{I.~Kavcic}
\author[met,read]{C.~M.~Maynard}
\ead{c.m.maynard@reading.ac.uk}
\author[met]{T.~Melvin}
\author[bath]{E.H.~Mueller}
\author[met]{S.~Mullerworth}
\author[hartree]{A.~Porter}
\author[downunder]{M.~Rezny}
\author[met]{B.~Shipway}
\author[met]{R.~Wong}




\address[met]{Met Office, FitzRoy Road, Exeter, EX1 3PB}
\address[read]{Department of Computer Science, Polly Vacher Building,
  University of Reading, Reading, UK, RG6 6AY}
\address[bath]{Department of Mathematics, University of Bath, Bath}
\address[downunder]{University of Monash, Melbourne, Australia}
\address[hartree]{Hartree Centre, STFC Daresbury, Grim up North}

\begin{abstract}
\end{abstract}

\begin{keyword}

\end{keyword}

\end{frontmatter}

\section{Introduction}
In common with many science applications, Exascale computing presents
a disruptive change for weather and climate models. However, the
difficulty in porting and optimising legacy codes to new hardware is
particular acute for this domain as the software is large ($O(10^6)$
lines of code), takes a long time develop ($\sim 10$ years for a new
dynamical core) and is long-lived (typically $\sim 25$ years or longer). These
timescales are much longer than the changes in both processor
architectures and programming models necessary to exploit these
architectures~\cite{gmd-2017-186}. Moreover, highly scalable
algorithms are necessary to exploit the necessary degree of
parallelism Exascale computers are likely exhibit.

In collaboration with academic partners, the Met Office is developing
a new dynamical core, called Gung Ho~\cite{MELVIN2018342}. By
employing a mixed Finite Element Method on an unstructured mesh, the
new dynamical core is designed to maintain the scientific accuracy of
the current Unified Model (UM) dycore (ENDGame~\cite{QJ:QJ2235}),
whilst allowing improved scalability by avoiding the singular pole of
the lon-lat grid. A new atmospheric model and software infrastructure,
named LFRic after Lewis Fry Richardson, is being developed to host the
Gung Ho dynamical core, as the the structured, lon-lat grid is inherent in the
data structures of the UM.

The software design is based on a layered architecture and a 
{\em separation of concerns} between the natural science code in which
the mathematics is expressed and the computational science code where the
parallelism and other performance optimisations are expressed. In particular,
there are three layers. The top layer, the algorithm layer, is where high-level mathematical 
operations on global fields are performed. The bottom layer is the kernel layer
where these operations are expressed on a single column of data. In between is the
Parallelisation System or PSy layer, where the horizontal looping and parallelism is
expressed. This abstraction, called PSyKAl, is written in Fortran with Fortran 2003
Object Orientation to encode the rules of the API.
Moreover, a Python code called Psyclone can parse the algorithm and kernel layers and
generate the Psy layer with different target programming models. In effect, the PSyKAl API
and Psyclone are a Domain Specific Embedded Language (DESL). Natural science code which
conforms to this API can be written in serial and the parallel code is then generated automatically.

The model is under active development and indeed, the science and
numerical analysis are still areas of active research. However, in
order to assess the scientific performance of the model, sufficiently
computationally challenging problems must be tackled. Thus, the
ability to generate optimisations for current architectures is also
required. 

The paper is organised as follows, the Gung Ho dynamical core and
computational aspects are presented in section~\ref{sec:GH}. The
software design for the separation of concerns and PSKAl API are described 
are described in section~\ref{sec:SoC}. The model infrastructure and
use of libraries is discussed in section~\ref{sec:lib}. PSyclone, the
code generator is presented in section~\ref{sec:Psyclone}. Finally a
scaling analysis is presented in section~\ref{sec:scal} and
conclusions drawn in section~\ref{sec:con}.

\section{\label{sec:GH}Gung Ho}
A bit about Gung Ho, but mostly the unstructured mesh, quads, 2+1D
mesh, vertically structured. Etc.

\section{\label{sec:SoC}Separation of Concerns}
Science applications in general and weather and climate codes in
particular are written in high-level languages such as Fortran or
C/C++. Fortran is commonly employed for weather and climate codes and
can be considered a Domain Specific Language (DSL) for numerical
computation.  The code is structured to solve a mathematical problem
as an alogorithm and in principle, separate from hardware architecural
concerns. The compiler generates machine specific instructions from
code which conforms to the langauge standard and can, in principle,
make optimisation choices to exploit the architecture of different
processors. 

This abstraction of a separation of concerns between mathematics code
and machine code is powerful and allows for both performance and
portabilty of science codes.

\section{\label{sec:lib}Infrastructure}.
ESMF, YAXT, XIOS, pFunit? netcdf etc.

\section{\label{sec:Psyclone}PSyclone}.
The code generator.

\section{\label{sec:Solver}Linear solvers and preconditioners}
Iterative solvers for large sparse linear systems of equations are required in several places in the model. For example, since mass matrices in the finite element discretisation are not diagonal, they need to be inverted with a small number of iterations of a Krylov subspace method. More importantly, the semi-implicit timestepping approach requires the solution of a very large sparse system for all prognostic unknowns in each step of the non-linear Picard iteration. Since the system is ill-conditioned, it needs to be preconditioned efficiently. This is achieved by the (approximate) reduction to an elliptic system for the pressure, which is preconditioned with a tensor-product multigrid algorithm \cite{Borm2001} (see Section \ref{sec:preconditioner}). To increase efficiency, the pressure preconditioner can be wrapped in its own iterative solver for the Helmholtz system. Note that in contrast to the  approach already employed in the ENDGame model \cite{QJ:QJ2235}, an outer iteration over the full system is still required due to the non-diagonal nature of the finite element mass matrices. Altogether this results in a rather complex solver.
\subsection{Solver infrastructure}
To allow easy implementation of sophisticaed nested iterative solvers and preconditioners, a dedicated abstraction was developed by using advanced object oriented features of Fortran 2003. This approach follows similar design philosophies in widely used linear algebra libraries such as PETSc \cite{Balay1997,Balay2018} and DUNE-ISTL \cite{Blatt2007}. More specifically, the implementation in LFRic uses derived types which realise the following operations (see Fig. \ref{fig:class_hierarchy}, left):
\begin{itemize}
\item \textbf{Vector} types which support common linear algebra operations such as AXPY $y\mapsto y+\alpha x$ and dot products $x,y\mapsto s = \langle x,y\rangle$. The most important vector-type is \texttt{field\_vector}, which contains a collection of model fields.
\item \textbf{Linear operator} types which implement the operation $x\mapsto y=Ax$ for vectors $x$ and $y$
\item \textbf{Preconditioners} which approximately solve the system $Px=b$ for a
  given right hand side $b$ and some operator $P\approx A$
\item \textbf{Iterative solvers} which solve the system $Ax=b$ with a Krylov-subspace method for a given right hand side $b$. Currently supported solvers include Conjugate Gradient, GMRES and BiCGStab.
\end{itemize}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.45\linewidth]{class_hierarchy.pdf}
    \hfill
    \includegraphics[width=0.45\linewidth]{class_concrete.pdf}
    \caption{Derived class hierarchy for solvers and preconditioners in LFric (left) and conctrete implementation for the linear system in implicit timestepping (right).}
    \label{fig:class_hierarchy}
  \end{center}
\end{figure}
Each of those types is derived from an abstract base type. The iterative solver types operate on generic vector types and are passed preconditioner and linear operator objects which adhere to the interface of their abstract base types.  This implies that only one instance of a particular Krylov method has to be implemented in the code. Apart from avoiding code duplication, this increases reliability and maintainability, since only one instance of each solver has to be developed and tested. In addition, it allows easy ``plug-and-play'' to explore the space of all possible solver/preconditioner combinations to achieve optimal performance.

To solve a particular problem, the user has to develop bespoke derived types for the corresponding linear operator and preconditioners. Note that the \texttt{apply()} methods of those derived types contain invoke kernels, which guarantees optimal performance of the entire model. For example, to construct a solver for the implicit linar system which is inverted in every semi-implicit timestep, the user implements the following objects (see Fig. \ref{fig:class_hierarchy}, right):
\begin{itemize}
\item A linear operator type which applies the full linear system to all components of a \texttt{field\_vector}.
\item A preconditioner type which reduces the full linear system to the (approximate) Schur complement in pressure space by lumping the velocity mass matrix; this preconditioner then calls the solver for the pressure (Helmholtz-) system.
\item A linear operator type which applies the Helmholtz-operator for the pressure system.
\item A preconditioner type which approximately inverts the Helmholtz operator (see section \ref{sec:preconditioner}).
\end{itemize}
Once implemented, those linear operators/preconditioners need to be passed to suitable existing linear solvers.

In addition to this more traditional approximate Schur-complement approach for solving the full linear system, the development of solvers based on a hybridisation approach is currently explored. Since an exact Schur-complement can be formed in this case, hybridisation avoids the expensive iteration over the full linear system, and potentially leads to significant improvements in performance.
\subsection{\label{sec:preconditioner}Preconditioner}
To precondition the strongly anisotropic Helmholtz-operator for the pressure system, the tensor-product multigrid approach in \cite{Borm2001} is used in LFric. This is a more advanced solver than the current tridiagonal vertical-only preconditioner in the ENDGame model. The multigrid algorithm used in LFRic has been tested extensively for representative elliptic equations in atmospheric modelling in \cite{Mueller2014,Dedner2016}, including a mixed-finite element discretisation of a linear gravity wave system in \cite{Mitchell2016}. The key idea to address the strong vertical anisotropy due to the high-aspect ratio of the domain is to combine vertical-only smoothing (line relaxation) with a horizontal multigrid hierarchy. To allow the easy construction of the vertical-only operators in the Schur-complement from the finite element discretisation of the full equations, a suitable operator algebra was developed in \cite{Mitchell2016} and tested in the Firedrake library. For horizontally discontinuous function spaces (such as the pressure space and the vertical-only velocity), operators can be assembled into a partially assembled matrix type, which stores all couplings within one vertical column. Matrices of this type can be multiplied, added and, most importantly, inverted in the tridiagonal solvers which realises the vertical line relaxation. This allows the high-level construction of the building blocks required for the Helmholtz-solver and preconditioner. The same data structures were implemented as derived Fortran 2003 types in the LFric code and form the building blocks of the tensor-product multigrid preconditioner for the elliptic Helmholtz system.

Compared to simpler preconditioners, which do not combine vertical line relaxation with a horizontal multigrid hierarchy, the tensor-product multigrid approach typically reduces the solution time of the pressure system by a factor of at least two \cite{Mueller2014,Mitchell2016}. However, since the Helmholtz system contains a zero-order term, only a small number ($\approx 3-4$) multigrid levels is required (independent of the grid resolution). This greatly increases scalability since it avoids global couplings which arise on the coarsest level.
\section{\label{sec:scal}Scaling}.
Scaling, MPI and OpenMP etc.

\section{Conclusion}
\label{sec:con}
Conclusions, future work etc


\bibliography{mibib.bib}

\end{document}
