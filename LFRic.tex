\documentclass[times]{elsarticle}
\bibliographystyle{elsarticle-num}
\usepackage{graphicx,colordvi}
\begin{document}
\begin{frontmatter}

\title{LFRic and PSyclone: Building a Domain Specific Language for Weather and Climate models}

\author[met]{S.~Adams}
\author[hartree]{R.~Ford}
\author[met]{M.~Hambley}
\author[met]{J.M.~Hobson}
\author[met]{I.~Kavcic}
\author[met,read]{C.~M.~Maynard}
\ead{c.m.maynard@reading.ac.uk}
\author[met]{T.~Melvin}
\author[bath]{E.H.~Mueller}
\author[met]{S.~Mullerworth}
\author[hartree]{A.~Porter}
\author[downunder]{M.~Rezny}
\author[met]{B.~Shipway}
\author[met]{R.~Wong}




\address[met]{Met Office, FitzRoy Road, Exeter, EX1 3PB}
\address[read]{Department of Computer Science, Polly Vacher Building,
  University of Reading, Reading, UK, RG6 6AY}
\address[bath]{Department of Mathematics, University of Bath, Bath}
\address[downunder]{Monash University, Australia}
\address[hartree]{Hartree Centre, STFC Daresbury, Grim up North}

\begin{abstract}
\end{abstract}

\begin{keyword}

\end{keyword}

\end{frontmatter}

\section{Introduction}
In common with many science applications, Exascale computing presents
a disruptive change for weather and climate models. However, the
difficulty in porting and optimising legacy codes to new hardware is
particular acute for this domain as the software is large ($O(10^6)$
lines of code), takes a long time develop ($\sim 10$ years for a new
dynamical core) and is long-lived (typically $\sim 25$ years or longer). These
timescales are much longer than the changes in both processor
architectures and programming models necessary to exploit these
architectures~\cite{gmd-2017-186}. Moreover, highly scalable
algorithms are necessary to exploit the necessary degree of
parallelism Exascale computers are likely exhibit.

In collaboration with academic partners, the Met Office is developing
a new dynamical core, called Gung Ho~\cite{MELVIN2018342}. By
employing a mixed Finite Element Method on an unstructured mesh, the
new dynamical core is designed to maintain the scientific accuracy of
the current Unified Model (UM) dycore (ENDGame~\cite{QJ:QJ2235}),
whilst allowing improved scalability by avoiding the singular pole of
the lon-lat grid. A new atmospheric model and software infrastructure,
named LFRic after Lewis Fry Richardson, is being developed to host the
Gung Ho dynamical core, as the the structured, lon-lat grid is inherent in the
data structures of the UM.

The software design is based on a layered architecture and a 
{\em separation of concerns} between the natural science code in which
the mathematics is expressed and the computational science code where the
parallelism and other performance optimisations are expressed. In particular,
there are three layers. The top layer, the algorithm layer, is where high-level mathematical 
operations on global fields are performed. The bottom layer is the kernel layer
where these operations are expressed on a single column of data. In between is the
Parallelisation System or PSy layer, where the horizontal looping and parallelism is
expressed. This abstraction, called PSyKAl, is written in Fortran with Fortran 2003
Object Orientation to encode the rules of the API.
Moreover, a Python code called Psyclone can parse the algorithm and kernel layers and
generate the Psy layer with different target programming models. In effect, the PSyKAl API
and Psyclone are a Domain Specific Embedded Language (DESL). Natural science code which
conforms to this API can be written in serial and the parallel code is then generated automatically.

The model is under active development and indeed, the science and
numerical analysis are still areas of active research. However, in
order to assess the scientific performance of the model, sufficiently
computationally challenging problems must be tackled. Thus, the
ability to generate optimisations for current architectures is also
required. 

The paper is organised as follows, the Gung Ho dynamical core and
computational aspects are presented in section~\ref{sec:GH}. The
software design for the separation of concerns and PSKAl API are described 
are described in section~\ref{sec:SoC}. The model infrastructure and
use of libraries is discussed in section~\ref{sec:lib}. PSyclone, the
code generator is presented in section~\ref{sec:Psyclone}. Finally a
scaling analysis is presented in section~\ref{sec:scal} and
conclusions drawn in section~\ref{sec:con}.

\section{\label{sec:GH}Gung Ho}
A bit about Gung Ho, but mostly the unstructured mesh, quads, 2+1D
mesh, vertically structured. Etc.

\section{\label{sec:SoC}Separation of Concerns}
Science applications in general and weather and climate codes in
particular are written in high-level languages such as Fortran or
C/C++. Fortran is commonly employed for weather and climate codes and
can be considered a Domain Specific Language (DSL) for numerical
computation.  The code is structured to solve a mathematical problem
as an alogorithm and in principle, separate from hardware architecural
concerns. The compiler generates machine specific instructions from
code which conforms to the langauge standard and can, in principle,
make optimisation choices to exploit the architecture of different
processors. 

This abstraction of a separation of concerns between mathematics code
and machine code is powerful and allows for both performance and
portabilty of science codes.

\section{\label{sec:lib}Infrastructure}.
ESMF, YAXT, XIOS, pFunit? netcdf etc.

\subsection{\label{sec:io}Parallel I/O}

A consequence of models running over many thousands of cores is a requirement
for scalable parallel I/O. A computationally optimised model is of little practical
operational use if I/O then becomes a limiting factor. The LFRic project has
decided to investigate options for I/O alongside the computational
and infrastructure development for three main reasons. Firstly, the requirement
to be able to run science assessment jobs on large numbers of cores and obtain the
output efficiently. Secondly, a key aim of LFRic is scalability and therefore it is
helpful to be able to monitor the impact of I/O on this as the infrastructure and
science develops. Thirdly, LFRic is making some fundamental changes that impact the
underlying mesh and this links to I/O in terms of input / output file formats. Having
a concrete idea of what these file formats will be in the future enables LFRic to
inform future users internal and external to the Met Office.
Although developing a bespoke I/O system was a possibility, the decision was
made early on to adopt an existing parallel I/O framework and leverage knowledge and
experience from the community; developing Earth System Models is becoming increasingly
complex and challenging for any one organisation to develop and maintain all the required
components~\cite{gmd-2017-186}.
As of early 2016, when the first scoping work for LFRic parallel I/O was being done,
there were several existing parallel I/O frameworks and XIOS was selected as the prime
candidate for evaluation~\cite{XIOSWiki}. A discussion of the other frameworks and why
they were not considered can be found in~\cite{Adams2018}.
XIOS was an obvious choice for LFRic as it was a mature framework, having had many years
of development and was already in use in the weather and climate domain. For example,
the Orchidee land surface model, the Dynamico dynamical core (part of the LMD-Z
Atmosphere model) and in the NEMO ocean model. NEMO is used in Met Office coupled
models so there was some existing experience with XIOS. In terms of potential scalability,
XIOS had also been proved to run successfully in models running on the order of 10,000
cores. Crucially for climate models, XIOS works with the OASIS coupling framework that
is commonly used in coupled climate models including the UM. 

The key features of XIOS can be summarised as follows:
\begin{enumerate}
  \item Flexible definition of I/O workflows via external XML files
  \item Client/Server architecture allowing asynchronous I/O servers on dedicated cores
  \item Sophisticated "in situ" post-processing via workflows defined in the XML file(s) 
\end{enumerate}

XIOS is written in C++, but also provides a Fortran API to developers. It makes use of
MPI-IO and the NetCDF4 and HDF5 libraries and handles unstructured and regular grids.
XIOS is a client-server framework where I/O servers are asynchronous processes buffering
output requests from client processes. XIOS also has sophisticated post-processing
functionality - e.g. for regridding, time series, meaning. The output schedule and
file format are defined by an XML file which hides a lot of complexity from the user.

Prior to 2016, XIOS only supported read and write of NetCDF file formats that follow the CF
conventions.  As previously explained in section~\ref{sec:GH}, the LFRic implementation uses
semi-structured meshes and a FEM formulation, where variables are held on
different elements of the mesh (nodes, edges or faces). Therefore, the I/O system needs to
handle data structured in this way. The UGRID-NetCDF format has been chosen as the main
LFRic input mesh format and also output format for diagnostics and model dumps~\cite{UgridSpec}.
In the UGRID convention the topology of the underlying unstructured mesh is stored as well as
data, and data can be defined on any of the mesh elements: nodes, edges, or faces.
Working in collaboration with IPSL (Institut Pierre Simon LaPlace), the ability to write the
UGRID format was added to XIOS in 2016 in order to support LFRic. 

XIOS has been integrated into LFRic via a lightweight I/O interface so that (as far as
possible) the underlying I/O framework is hidden and could be replaced by an alternative.
The I/O interface is designed to be compatible with the use of Fortran 2003 Object
Orientation in LFRic. The field object \textbf{(refer to relevant infrastructure section when 
this is written)} contains read and write interface declarations that use Fortran 
procedure pointers to define I/O 'behaviours'. When a field is created the behaviours
are set by pointing to a specific reading or writing method. Writing a field just involves
calling $field\%write\_field()$ - i.e. it is the responsibility of the field to write
itself and how the writing is actually done is hidden.  

More details of the experimental results of the LFRic-XIOS integration work can be
found in~\cite{Adams2018} where preliminary results on I/O scalability (up to 13824 
processors) and XIOS and Lustre performance tuning are presented. The conclusions of
this work were that XIOS is an effective parallel I/O framework that was technically
straightforward to integrate into the existing LFRic infrastructure via the XIOS
Fortran interface. Scaling experiments showed that XIOS performance was scaleable with
respect to increasing numbers of cores, even with minimal tuning.

\section{\label{sec:Psyclone}PSyclone}.
The code generator.

\section{\label{sec:scal}Scaling}.
Scaling, MPI and OpenMP etc.

\section{Conclusion}
\label{sec:con}
Conclusions, future work etc


\bibliography{mibib.bib}

\end{document}
